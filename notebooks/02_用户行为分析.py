# -*- coding: utf-8 -*-
"""
02_ç”¨æˆ·è¡Œä¸ºåˆ†æ.py
æ·±å…¥çš„ç”¨æˆ·è¡Œä¸ºåˆ†æï¼ŒåŒ…å«RFMæ¨¡å‹ã€è½¬åŒ–æ¼æ–—ã€ç”¨æˆ·åˆ†ç¾¤ç­‰
ä½¿ç”¨ #%% åˆ†éš”ä»£ç å—ï¼Œæ¨¡æ‹Ÿ Jupyter Notebook
"""

#%% å¯¼å…¥å¿…è¦çš„åº“
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import sys
import os
from datetime import datetime, timedelta

# æ·»åŠ srcç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'src'))
from analysis import EcommerceAnalyzer
from visualization import EcommerceVisualizer

# è®¾ç½®æ ·å¼
plt.style.use('seaborn-v0_8')
sns.set_palette("Set2")

print("ğŸ¯ ç”µå•†ç”¨æˆ·è¡Œä¸ºæ·±åº¦åˆ†æ")
print("=" * 50)

#%% åˆå§‹åŒ–åˆ†æå™¨å’Œå¯è§†åŒ–å™¨
print("\nğŸ”§ åˆå§‹åŒ–åˆ†æå·¥å…·...")

# åˆ›å»ºåˆ†æå™¨å’Œå¯è§†åŒ–å™¨å®ä¾‹
analyzer = EcommerceAnalyzer()
visualizer = EcommerceVisualizer()

# åŠ è½½æ•°æ®
analyzer.load_data()
print("âœ… æ•°æ®åŠ è½½å®Œæˆï¼")

#%% åŸºç¡€ç»Ÿè®¡åˆ†æ
print("\nğŸ“Š åŸºç¡€ç»Ÿè®¡åˆ†æ")
print("-" * 30)

# è·å–åŸºç¡€ç»Ÿè®¡
basic_stats = analyzer.basic_stats()
print("ã€å¹³å°æ ¸å¿ƒæŒ‡æ ‡ã€‘")
for metric, value in basic_stats.items():
    if 'é‡‘é¢' in metric:
        print(f"{metric}: {value:,.2f} å…ƒ")
    elif 'æ•°' in metric:
        print(f"{metric}: {value:,}")
    else:
        print(f"{metric}: {value}")

# è®¡ç®—å…³é”®ä¸šåŠ¡æŒ‡æ ‡
conversion_rate = (basic_stats['è´­ä¹°ç”¨æˆ·æ•°'] / basic_stats['æ´»è·ƒç”¨æˆ·æ•°']) * 100
print(f"\nğŸ¯ å…³é”®æŒ‡æ ‡:")
print(f"ç”¨æˆ·è½¬åŒ–ç‡: {conversion_rate:.2f}%")
print(f"äººå‡è®¢å•é‡‘é¢: {basic_stats['å¹³å‡è®¢å•é‡‘é¢']:.2f}å…ƒ")
print(f"æ´»è·ƒç”¨æˆ·å æ¯”: {(basic_stats['æ´»è·ƒç”¨æˆ·æ•°'] / basic_stats['ç”¨æˆ·æ€»æ•°']) * 100:.1f}%")

#%% RFMç”¨æˆ·ä»·å€¼åˆ†æ
print("\nğŸ’ RFMç”¨æˆ·ä»·å€¼åˆ†æ")
print("-" * 30)

# æ‰§è¡ŒRFMåˆ†æ
rfm_results = analyzer.rfm_analysis()
print("âœ… RFMåˆ†æå®Œæˆï¼")

# RFMåˆ†å¸ƒç»Ÿè®¡
print("\nã€RFMæŒ‡æ ‡åˆ†å¸ƒã€‘")
print(f"æœ€è¿‘è´­ä¹°æ—¶é—´ (Recency):")
print(f"  å¹³å‡: {rfm_results['recency'].mean():.1f} å¤©")
print(f"  ä¸­ä½æ•°: {rfm_results['recency'].median():.1f} å¤©")
print(f"  èŒƒå›´: {rfm_results['recency'].min()}-{rfm_results['recency'].max()} å¤©")

print(f"\nè´­ä¹°é¢‘æ¬¡ (Frequency):")
print(f"  å¹³å‡: {rfm_results['frequency'].mean():.1f} æ¬¡")
print(f"  ä¸­ä½æ•°: {rfm_results['frequency'].median():.1f} æ¬¡")
print(f"  æœ€é«˜: {rfm_results['frequency'].max()} æ¬¡")

print(f"\næ¶ˆè´¹é‡‘é¢ (Monetary):")
print(f"  å¹³å‡: {rfm_results['monetary'].mean():.2f} å…ƒ")
print(f"  ä¸­ä½æ•°: {rfm_results['monetary'].median():.2f} å…ƒ")
print(f"  æœ€é«˜: {rfm_results['monetary'].max():.2f} å…ƒ")

# ç”¨æˆ·åˆ†ç¾¤åˆ†æ
print("\nã€ç”¨æˆ·åˆ†ç¾¤ç»“æœã€‘")
segment_analysis = rfm_results['customer_segment'].value_counts()
segment_pct = rfm_results['customer_segment'].value_counts(normalize=True) * 100

for segment, count in segment_analysis.items():
    pct = segment_pct[segment]
    avg_monetary = rfm_results[rfm_results['customer_segment'] == segment]['monetary'].mean()
    print(f"{segment}: {count} äºº ({pct:.1f}%) - å¹³å‡æ¶ˆè´¹ {avg_monetary:.2f}å…ƒ")

# ä¿å­˜RFMç»“æœ
os.makedirs('data/processed', exist_ok=True)
rfm_results.to_csv('data/processed/rfm_analysis.csv', index=False)
print(f"\nğŸ’¾ RFMåˆ†æç»“æœå·²ä¿å­˜")

#%% è½¬åŒ–æ¼æ–—åˆ†æ
print("\nğŸ”„ è½¬åŒ–æ¼æ–—åˆ†æ")
print("-" * 30)

# æ‰§è¡Œæ¼æ–—åˆ†æ
funnel_results = analyzer.funnel_analysis()
print("âœ… æ¼æ–—åˆ†æå®Œæˆï¼")

# æ˜¾ç¤ºæ¼æ–—ç»“æœ
print("\nã€è½¬åŒ–æ¼æ–—è¯¦æƒ…ã€‘")
for _, row in funnel_results.iterrows():
    step_name = row['step_name']
    user_count = row['user_count']
    conversion_rate = row['conversion_rate']
    step_conversion = row['step_conversion_rate']
    
    print(f"{step_name}:")
    print(f"  ç”¨æˆ·æ•°: {user_count:,}")
    print(f"  æ€»è½¬åŒ–ç‡: {conversion_rate:.2f}%")
    if step_conversion > 0:
        print(f"  æ­¥éª¤è½¬åŒ–ç‡: {step_conversion:.2f}%")
    print()

# æ‰¾å‡ºæœ€å¤§æµå¤±ç‚¹
step_conversions = funnel_results['step_conversion_rate'][1:]  # æ’é™¤ç¬¬ä¸€æ­¥
if len(step_conversions) > 0:
    min_conversion_idx = step_conversions.idxmin()
    worst_step = funnel_results.iloc[min_conversion_idx]['step_name']
    worst_rate = funnel_results.iloc[min_conversion_idx]['step_conversion_rate']
    print(f"ğŸš¨ å…³é”®æµå¤±ç‚¹: {worst_step} (è½¬åŒ–ç‡ä»… {worst_rate:.1f}%)")

# ä¿å­˜æ¼æ–—ç»“æœ
funnel_results.to_csv('data/processed/funnel_analysis.csv', index=False)

#%% ç”¨æˆ·è¡Œä¸ºæ¨¡å¼åˆ†æ
print("\nğŸ“± ç”¨æˆ·è¡Œä¸ºæ¨¡å¼åˆ†æ")
print("-" * 30)

# æ‰§è¡Œç”¨æˆ·è¡Œä¸ºåˆ†æ
behavior_analysis = analyzer.user_behavior_analysis()
print("âœ… è¡Œä¸ºæ¨¡å¼åˆ†æå®Œæˆï¼")

# è®¾å¤‡ä½¿ç”¨åˆ†æ
print("\nã€è®¾å¤‡ä½¿ç”¨åˆ†æã€‘")
device_data = behavior_analysis['device_analysis']
for device, data in device_data.iterrows():
    unique_users = data['unique_users']
    total_behaviors = data['total_behaviors']
    avg_behaviors = total_behaviors / unique_users
    print(f"{device}:")
    print(f"  ç‹¬ç«‹ç”¨æˆ·: {unique_users:,}")
    print(f"  è¡Œä¸ºæ€»æ•°: {total_behaviors:,}")
    print(f"  äººå‡è¡Œä¸º: {avg_behaviors:.1f} æ¬¡")

# æ—¶é—´åˆ†å¸ƒåˆ†æ
print("\nã€ç”¨æˆ·æ´»è·ƒæ—¶é—´åˆ†æã€‘")
hourly_data = behavior_analysis['hourly_analysis']
peak_hour = hourly_data.idxmax()
peak_count = hourly_data.max()
print(f"æœ€æ´»è·ƒæ—¶æ®µ: {peak_hour}:00 ({peak_count:,} æ¬¡è¡Œä¸º)")

# ç»Ÿè®¡æ—¶æ®µåˆ†å¸ƒ
morning = hourly_data[6:12].sum()    # 6-12ç‚¹
afternoon = hourly_data[12:18].sum() # 12-18ç‚¹
evening = hourly_data[18:24].sum()   # 18-24ç‚¹
night = hourly_data[0:6].sum()       # 0-6ç‚¹

total_behaviors = morning + afternoon + evening + night
print(f"æ—¶æ®µåˆ†å¸ƒ:")
print(f"  ä¸Šåˆ (6-12ç‚¹): {morning:,} ({morning/total_behaviors*100:.1f}%)")
print(f"  ä¸‹åˆ (12-18ç‚¹): {afternoon:,} ({afternoon/total_behaviors*100:.1f}%)")
print(f"  æ™šä¸Š (18-24ç‚¹): {evening:,} ({evening/total_behaviors*100:.1f}%)")
print(f"  æ·±å¤œ (0-6ç‚¹): {night:,} ({night/total_behaviors*100:.1f}%)")

# ç”¨æˆ·ç­‰çº§è¡Œä¸ºåˆ†æ
print("\nã€ç”¨æˆ·ç­‰çº§è¡Œä¸ºå¯¹æ¯”ã€‘")
level_behavior = behavior_analysis['user_level_analysis']
for level in level_behavior.index:
    total_level_behaviors = level_behavior.loc[level].sum()
    print(f"{level}ç”¨æˆ·:")
    for behavior_type in level_behavior.columns:
        count = level_behavior.loc[level, behavior_type]
        pct = count / total_level_behaviors * 100 if total_level_behaviors > 0 else 0
        print(f"  {behavior_type}: {count} ({pct:.1f}%)")

#%% ç”¨æˆ·ç•™å­˜åˆ†æ
print("\nğŸ“ˆ ç”¨æˆ·ç•™å­˜åˆ†æ")
print("-" * 30)

try:
    # æ‰§è¡Œç•™å­˜åˆ†æ
    cohort_results = analyzer.cohort_analysis()
    print("âœ… ç•™å­˜åˆ†æå®Œæˆï¼")
    
    # åˆ†æç•™å­˜è¶‹åŠ¿
    if not cohort_results.empty:
        print("\nã€ç•™å­˜ç‡è¶‹åŠ¿ã€‘")
        # è®¡ç®—å„æœˆåº¦çš„å¹³å‡ç•™å­˜ç‡
        for month in range(min(6, cohort_results.shape[1])):
            if month in cohort_results.columns:
                avg_retention = cohort_results[month].mean()
                print(f"ç¬¬{month}ä¸ªæœˆç•™å­˜ç‡: {avg_retention:.2%}")
        
        # ä¿å­˜ç•™å­˜ç»“æœ
        cohort_results.to_csv('data/processed/cohort_analysis.csv')
        print(f"\nğŸ’¾ ç•™å­˜åˆ†æç»“æœå·²ä¿å­˜")
    else:
        print("âš ï¸  ç•™å­˜æ•°æ®ä¸è¶³ï¼Œè·³è¿‡åˆ†æ")
        
except Exception as e:
    print(f"âš ï¸  ç•™å­˜åˆ†æé‡åˆ°é—®é¢˜: {e}")

#%% ç”¨æˆ·ç”Ÿå‘½å‘¨æœŸä»·å€¼åˆ†æ
print("\nğŸ’° ç”¨æˆ·ç”Ÿå‘½å‘¨æœŸä»·å€¼åˆ†æ")
print("-" * 30)

# è®¡ç®—ç”¨æˆ·LTV (ç®€åŒ–ç‰ˆ)
orders_df = analyzer.orders
users_df = analyzer.users

# åˆå¹¶è®¢å•å’Œç”¨æˆ·æ•°æ®
user_orders = orders_df.merge(users_df[['user_id', 'register_date']], on='user_id')
user_orders['order_time'] = pd.to_datetime(user_orders['order_time'])
user_orders['register_date'] = pd.to_datetime(user_orders['register_date'])

# è®¡ç®—ç”¨æˆ·çš„ç”Ÿå‘½å‘¨æœŸæŒ‡æ ‡
user_ltv = user_orders.groupby('user_id').agg({
    'total_amount': ['sum', 'mean', 'count'],
    'order_time': ['min', 'max']
}).reset_index()

# æ‰å¹³åŒ–åˆ—å
user_ltv.columns = ['user_id', 'total_spent', 'avg_order_value', 'order_count', 'first_order', 'last_order']

# è®¡ç®—ç”Ÿå‘½å‘¨æœŸå¤©æ•°
user_ltv['lifecycle_days'] = (user_ltv['last_order'] - user_ltv['first_order']).dt.days + 1

# è¿‡æ»¤æœ‰æ•ˆæ•°æ®
valid_users = user_ltv[user_ltv['order_count'] > 0]

if len(valid_users) > 0:
    print("ã€ç”¨æˆ·ä»·å€¼åˆ†å¸ƒã€‘")
    print(f"è´­ä¹°ç”¨æˆ·æ€»æ•°: {len(valid_users)}")
    print(f"å¹³å‡è®¢å•ä»·å€¼: {valid_users['avg_order_value'].mean():.2f}å…ƒ")
    print(f"å¹³å‡è´­ä¹°æ¬¡æ•°: {valid_users['order_count'].mean():.1f}æ¬¡")
    print(f"å¹³å‡ç”Ÿå‘½å‘¨æœŸ: {valid_users['lifecycle_days'].mean():.1f}å¤©")
    print(f"å¹³å‡ç”¨æˆ·ä»·å€¼: {valid_users['total_spent'].mean():.2f}å…ƒ")
    
    # ä»·å€¼åˆ†å±‚
    high_value_threshold = valid_users['total_spent'].quantile(0.8)
    medium_value_threshold = valid_users['total_spent'].quantile(0.5)
    
    high_value_users = len(valid_users[valid_users['total_spent'] >= high_value_threshold])
    medium_value_users = len(valid_users[
        (valid_users['total_spent'] >= medium_value_threshold) & 
        (valid_users['total_spent'] < high_value_threshold)
    ])
    low_value_users = len(valid_users[valid_users['total_spent'] < medium_value_threshold])
    
    print(f"\nã€ç”¨æˆ·ä»·å€¼åˆ†å±‚ã€‘")
    print(f"é«˜ä»·å€¼ç”¨æˆ· (>{high_value_threshold:.0f}å…ƒ): {high_value_users} ({high_value_users/len(valid_users)*100:.1f}%)")
    print(f"ä¸­ä»·å€¼ç”¨æˆ· ({medium_value_threshold:.0f}-{high_value_threshold:.0f}å…ƒ): {medium_value_users} ({medium_value_users/len(valid_users)*100:.1f}%)")
    print(f"ä½ä»·å€¼ç”¨æˆ· (<{medium_value_threshold:.0f}å…ƒ): {low_value_users} ({low_value_users/len(valid_users)*100:.1f}%)")
    
    # ä¿å­˜LTVåˆ†æ
    valid_users.to_csv('data/processed/user_ltv_analysis.csv', index=False)

#%% å•†å“åˆ†æ
print("\nğŸ›ï¸ å•†å“åˆ†æ")
print("-" * 30)

# å•†å“å—æ¬¢è¿ç¨‹åº¦åˆ†æ
behaviors_df = analyzer.behaviors
products_df = analyzer.products

# å•†å“è¡Œä¸ºç»Ÿè®¡
product_stats = behaviors_df.groupby('product_id').agg({
    'user_id': 'nunique',
    'behavior_type': 'count'
}).rename(columns={'user_id': 'unique_users', 'behavior_type': 'total_interactions'})

# åˆå¹¶å•†å“ä¿¡æ¯
product_analysis = product_stats.merge(products_df, on='product_id', how='left')

# åˆ†ææœ€å—æ¬¢è¿çš„å•†å“ç±»åˆ«
category_popularity = product_analysis.groupby('category').agg({
    'unique_users': 'sum',
    'total_interactions': 'sum',
    'product_id': 'count'
}).rename(columns={'product_id': 'product_count'})

category_popularity['avg_interactions_per_product'] = (
    category_popularity['total_interactions'] / category_popularity['product_count']
)

print("ã€å•†å“ç±»åˆ«å—æ¬¢è¿ç¨‹åº¦ (å‰10)ã€‘")
top_categories = category_popularity.sort_values('unique_users', ascending=False).head(10)
for category, data in top_categories.iterrows():
    print(f"{category}:")
    print(f"  ç‹¬ç«‹ç”¨æˆ·: {data['unique_users']:,}")
    print(f"  æ€»äº¤äº’: {data['total_interactions']:,}")
    print(f"  å•†å“æ•°é‡: {data['product_count']}")
    print(f"  å¹³å‡æ¯å•†å“äº¤äº’: {data['avg_interactions_per_product']:.1f}")

#%% ç”Ÿæˆä¸šåŠ¡æ´å¯Ÿ
print("\nğŸ’¡ ä¸šåŠ¡æ´å¯Ÿä¸å»ºè®®")
print("-" * 30)

# ä½¿ç”¨åˆ†æå™¨ç”Ÿæˆæ´å¯Ÿ
insights = analyzer.generate_insights()

print("ã€æ•°æ®é©±åŠ¨çš„ä¸šåŠ¡æ´å¯Ÿã€‘")
for i, insight in enumerate(insights, 1):
    print(f"{i}. {insight}")

# è¡¥å……è¯¦ç»†å»ºè®®
print(f"\nã€å…·ä½“ä¼˜åŒ–å»ºè®®ã€‘")

# åŸºäºRFMåˆ†æçš„å»ºè®®
champion_ratio = (rfm_results['customer_segment'] == 'å† å†›ç”¨æˆ·').mean() * 100
churn_ratio = (rfm_results['customer_segment'] == 'æµå¤±ç”¨æˆ·').mean() * 100

if champion_ratio < 15:
    print("â€¢ å† å†›ç”¨æˆ·å æ¯”åä½ï¼Œå»ºè®®åŠ å¼ºç”¨æˆ·æ¿€åŠ±å’Œå¿ è¯šåº¦è®¡åˆ’")

if churn_ratio > 20:
    print("â€¢ æµå¤±ç”¨æˆ·å æ¯”è¾ƒé«˜ï¼Œå»ºè®®å®æ–½å¬å›ç­–ç•¥å’Œä¸ªæ€§åŒ–æ¨è")

# åŸºäºæ¼æ–—åˆ†æçš„å»ºè®®
min_conversion_step = funnel_results.loc[funnel_results['step_conversion_rate'].idxmin()]
if min_conversion_step['step_conversion_rate'] < 30:
    step_name = min_conversion_step['step_name']
    print(f"â€¢ {step_name}ç¯èŠ‚è½¬åŒ–ç‡è¿‡ä½ï¼Œå»ºè®®ä¼˜åŒ–ç”¨æˆ·ä½“éªŒå’Œæµç¨‹è®¾è®¡")

# åŸºäºæ—¶é—´åˆ†æçš„å»ºè®®
if evening > afternoon:
    print("â€¢ ç”¨æˆ·æ™šé—´æ´»è·ƒåº¦é«˜ï¼Œå»ºè®®åœ¨18-24ç‚¹å¢åŠ è¥é”€æ´»åŠ¨å’Œæ¨é€")

#%% ä¿å­˜åˆ†æç»“æœ
print("\nğŸ’¾ ä¿å­˜åˆ†æç»“æœ")
print("-" * 30)

# åˆ›å»ºç»¼åˆæŠ¥å‘Š
report_data = {
    'analysis_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
    'total_users': len(analyzer.users),
    'active_users': analyzer.behaviors['user_id'].nunique(),
    'conversion_rate': conversion_rate,
    'champion_users_pct': champion_ratio,
    'churn_users_pct': churn_ratio,
    'peak_hour': int(peak_hour),
    'avg_order_value': basic_stats['å¹³å‡è®¢å•é‡‘é¢'],
    'key_insights': insights
}

# ä¿å­˜ä¸ºJSONæ ¼å¼
import json
with open('data/processed/behavior_analysis_report.json', 'w', encoding='utf-8') as f:
    json.dump(report_data, f, ensure_ascii=False, indent=2)

print("âœ… ç”¨æˆ·è¡Œä¸ºåˆ†æå®Œæˆï¼")
print("\nğŸ“‹ åˆ†ææ€»ç»“:")
print(f"  - RFMç”¨æˆ·åˆ†ç¾¤: å† å†›ç”¨æˆ· {champion_ratio:.1f}%, æµå¤±ç”¨æˆ· {churn_ratio:.1f}%")
print(f"  - è½¬åŒ–ç‡: {conversion_rate:.2f}%")
print(f"  - ç”¨æˆ·æœ€æ´»è·ƒæ—¶æ®µ: {peak_hour}:00")
print(f"  - å¹³å‡è®¢å•é‡‘é¢: {basic_stats['å¹³å‡è®¢å•é‡‘é¢']:.2f}å…ƒ")
print(f"  - åˆ†ææŠ¥å‘Šå·²ä¿å­˜åˆ° data/processed/ ç›®å½•")

#%% ä¸‹ä¸€æ­¥è®¡åˆ’
print("\nğŸ¯ ä¸‹ä¸€æ­¥è®¡åˆ’")
print("-" * 30)
print("1. A/Bæµ‹è¯•è®¾è®¡å’Œåˆ†æ")
print("2. ç”¨æˆ·ç»†åˆ†ç­–ç•¥åˆ¶å®š")
print("3. ä¸ªæ€§åŒ–æ¨èæ–¹æ¡ˆ")
print("4. åˆ¶ä½œæ•°æ®å¯è§†åŒ–Dashboard")
print("5. æ’°å†™æœ€ç»ˆä¸šåŠ¡æŠ¥å‘Š")

print(f"\n{'='*50}")
print("ğŸ¯ ç”¨æˆ·è¡Œä¸ºåˆ†æå®Œæˆï¼")
print("ğŸ‘‰ è¯·ç»§ç»­è¿è¡Œ 03_ABæµ‹è¯•åˆ†æ.py") 